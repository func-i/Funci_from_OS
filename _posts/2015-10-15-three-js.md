---
layout:     post
title:      Experimenting With Three.js and WebGL
author:     bruno
date:       2015-10-15
published:  true
social_image: blog_posts/2015-10-15-three-js/social
description: I recently graduated from Lighthouse Labs, and as my final project I worked with two other students to create a small music visualizer webapp, where users could choose songs from Soundcloud and create small music videos using our visualizer by creating transitions at specific points in time.
mail_list: something
mail_ask: Leave us your email
---

I recently graduated from Lighthouse Labs, and as my final project I worked with two other students to create a small music visualizer webapp,
where users could choose songs from [Soundcloud](https://soundcloud.com/) and create small music videos using our visualizer
by creating transitions at specific points in time.

<!--more-->

These transitions involved changes in post-processing effects, materials and geometry.

The first step to creating a music visualizer is obviously making your visuals react to the musical information in an
interesting and fully dynamic way. The simplest and most obvious approach is simply using the amplitude (time domain data) as a scaling,
rotation or translation coefficient.
The other obvious (and slightly more interesting) approach is to also use the frequency domain data as well,
having your visualization change based off the distribution of frequencies used. Some examples
include a [simple spectrogram](https://www.shadertoy.com/view/Xds3Rr) or this [weird ball thing](https://www.shadertoy.com/view/Xtl3W2),
the latter of which I found particularly interesting and exciting, so I sought to try and implement something similar in my project.

[Three.js](http://threejs.org/) is a '3D Javascript Library' that includes WebGL, \<canvas\>, \<svg\>, and more.
WebGL itself is a javascript API for rendering 3D in a \<canvas\> element. Three.js make is really easy to get set up and
started in making simple 3D scenes invloving lighting, shading and materials, particles etc. It's great. Interacting directly with the
WebGL API can be tedious and is typically very low level,
so having Three.js handle basic things like [Phong Shading](https://en.wikipedia.org/wiki/Phong_shading) and geometry loading is really great.

Following the following examples, I set out to impletment [spherical environment mapping](http://www.clicktorelease.com/blog/creating-spherical-environment-mapping-shader)
and [vertex displacement](http://www.clicktorelease.com/blog/vertex-displacement-noise-3d-webgl-glsl-three-js) as a function of perlin noise and the frequency spectrum.
To do this you're probably going to have to write your own GLSL shaders that describe this behaviour for each pixel/subpixel (fragment shader) and vertex (vertex shader).
Writing these shaders is still a bit of a strange experience, as you need to either write it as a string in your javascript code,
write it in as a script tag in your html and grab the text using the DOM or load it from another file using ajax (I chose to use the
html approach, it was easiest even if it resulted in having GLSL's C-like syntax in the middle of my html page).

To pass the frequency information in to my vertex shaders, I wrote the information to a texture using the array of values passed through
[WebAudio API's](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) analyzer node.
The reasoning behind this is that you can only pass a specific set of data types to your shaders as
[uniforms](https://www.opengl.org/wiki/Uniform_(GLSL)) (basically any input params that are considered immutable in the shader scope).
These can be things like 2D & 3D vectors, matrices, and textures, but must be of a specific type. It made sense to me to just access the
frequency information as if it were a low resolution discplacement map.

I quickly encountered (a now obvious) problem in that my SEM shading relied on the object's normal information,
which was passed in BEFORE any displacement occured. The naive solution is to just recalulate the normal vector for each vertex after the
displacement, but this is not really possible as the whole point of vertex and fragment shaders is that they can run in parallel on the GPU,
without needing any knowledge of adjacent verticies. My approach was to instead use the extension GL_OES_standard_derivatives, which would allow me to take the partial derivatives of
my new vertex positions in my fragment shader, but this ended breaking the smooth shading implementaion of my SEM shader.
This was fine in the end, as it produced interesting patterns, but it's definitely something to be aware of if you plan on implementing any sort of
dynamic vertex displacement. If I were to do this again, I would look into just creating a height map that properly maps to the UV coordinates of my geometry,
and then create an associated normal map whose values I could use in the fragment shader to preserve the smooth shading.

Either way, Three.js and WebGL are really exciting technologies that I'm pretty eager to experiment with more.
I've barely scratched the surface of what the library has to offer (I'm particularly keen to test out the deferred lighting system)
and I'm very excited to see what new features are pulled into it in the future (as of this writing, it has 77 pull requests on
[GitHub](https://github.com/mrdoob/three.js/pulls?q=is%3Aopen+is%3Apr)


